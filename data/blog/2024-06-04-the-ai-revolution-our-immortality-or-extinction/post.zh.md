---
title: "人工智能革命：我们永生还是灭绝"
date: 2024-06-04 11:25
tags: [AI]
---

> 本文转载自 [Wait but why](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html)，作者 Tim Urban 于2015年1月27日发表。  
> GPT-4o 和 GPT-3.5-Turbo 翻译

（这是第二部分，阅读[本文的第一部分](2024-06-04-the-ai-revolution-the-road-to-superintelligence)）

*我们可能面临一个极其困难的问题，解决时间未知，但这个问题很可能关系到人类的整个未来。—* Nick Bostrom

欢迎来到“等等，我怎么会在读这个？我不明白为什么大家都不在讨论这个”的系列的第二部分。

第一部分的开始还算简单，我们讨论了人工狭义智能（ANI）（擅长单一任务的AI，比如规划驾驶路线或下棋），以及它在今天的世界中无处不在。然后我们探讨了从ANI到人工通用智能（AGI，在各方面至少与人类一样具有智力能力的AI）的巨大挑战，并讨论了过去我们看到的技术进步的指数增长率表明，AGI可能并不像看起来那么遥远。第一部分的结尾是我告诉你们，一旦我们的机器达到人类级别的智能，它们可能会立即这样做：

![Train1](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Train1.png)

![Train2](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Train2.png)

![Train3](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Train3.png)

![Train4](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Train4.png)

这让我们默默盯着屏幕，面对着在我们有生之年可能出现的人工超级智能（ASI）的激烈概念（在各方面都比任何人类聪明得多的AI），试图弄清楚在思考这个问题时我们应该有什么样的情绪。[^1b] [^1g] <sup>← 点这里</sup>

在深入探讨之前，让我们先想一下，人工超级智能意味着什么。

一个关键的区别是“*速度超级智能*”和“*质量超级智能*”之间的差异。通常，当人们想象一个超级智能的计算机时，首先想到的是一个与人类一样智能但思考速度*快得多*的计算机[^2b]——他们可能会想象一个像人类一样思考，但速度快一百万倍的机器，这意味着它可以在五分钟内想出人类需要十年才能解决的问题。

这听起来很令人印象深刻，而且ASI确实*会*比任何人类思考得更快——但真正的区别在于它在智力*质量*上的优势，这是完全不同的。使人类在智力上比黑猩猩更强大的不是思维速度的差异——而是人类大脑包含许多复杂的认知模块，这些模块使复杂的语言表示、长期规划或抽象推理等成为可能，而黑猩猩的大脑没有这些模块。即使把黑猩猩的大脑加速几千倍，也无法达到我们的水平——即使给它十年的时间，它也无法弄明白如何使用一套定制工具来组装一个复杂的模型，而人类可以在几个小时内完成。而且，有很多人类的认知功能是黑猩猩永远无法做到的，无论它花多少时间去尝试。

但不仅仅是黑猩猩不能做我们做的事情，它的大脑甚至无法理解这些世界的*存在*——黑猩猩可以熟悉人类是什么和摩天大楼是什么，但它永远无法*理解*摩天大楼是由人类建造的。在它的世界里，任何如此庞大的东西都是自然的一部分，绝对*不会想到它是由某人建造的*。这是智力质量上小小差异的结果。

在我们今天讨论的智力范围中，甚至在生物生物体之间的较小范围内，人类与黑猩猩之间的智力质量差距是*微不足道的*。在早期的[帖子](https://waitbutwhy.com/2014/10/religion-for-the-nonreligious.html)中，我用一个楼梯描绘了生物认知能力的范围：[^3b]

![staircase](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/staircase1.png)

要理解超级智能机器的重大意义，想象一下楼梯上比我们高两步的深绿色台阶上的一个机器。这个机器只是*稍微*超级智能，但它在认知能力上的优势与我们与黑猩猩之间的差距一样巨大。而且就像黑猩猩永远无法理解摩天大楼是可以建造的一样，即使机器试图向我们解释，我们也永远无法理解深绿色台阶上的机器可以做的事情，更不用说我们自己做到。而这只是比我们高两步的机器。楼梯上倒数第二高台阶上的机器对我们来说就像我们对蚂蚁一样——即使机器花费多年试图教我们它所知道的最简单的概念，这一尝试也是徒劳的。

但我们今天讨论的超级智能远远超出了这个楼梯上的任何东西。在智力爆炸中——机器越聪明，它提高自身智力的速度就越快，直到它开始*飙升*——机器可能需要多年才能从黑猩猩台阶上升到上面的台阶，但一旦它到达比我们高两步的深绿色台阶，可能只需要几个小时就能再跳上一步，而当它比我们高十步时，它可能每秒钟跳四步。因此，我们需要认识到，很可能在关于第一台达到人类级AGI的机器的大新闻发布后不久，我们可能面临在地球上与这种在楼梯上的东西共存的现实（或许比我们高一百万倍）：

![staircase2](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/staircase2.png)

既然我们已经确定，尝试理解比我们高两步的机器的能力是无望的活动，那我们就非常具体地说，**我们无法知道ASI会做什么或对我们会产生什么后果。**任何声称知道的人都不理解超级智能的含义。

进化在数亿年的时间里缓慢而逐渐地推动生物大脑的进化，从这个意义上说，如果人类创造出ASI机器，我们将大踏步地踩在进化的轨道上。或者这也许是进化的*部分*也许进化的方式是智力逐渐提高，直到达到能够创造出机器超级智能的水平，这个水平就像一个触发全球变革的引线，决定所有生物的新未来：

![Tripwire](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Tripwire.png)

出于我们稍后将讨论的原因，科学界的大多数人认为这不是是否会触发引线的问题，而是何时触发。这是一个非常疯狂的信息。

那么这让我们处于什么位置？

世界上没有人，尤其不是我，能告诉你当我们触发引线时会发生什么。但牛津大学哲学家和AI领域的主要思想家Nick Bostrom认为，我们可以将所有潜在结果归结为两个大类。

首先，从历史上看，我们可以看到，生命的运作方式是：物种出现，存在一段时间，最终不可避免地从存在的平衡木上跌落，陷入灭绝——

![beam1](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/beam1.png)

“所有物种最终都会灭绝”这一规则在历史上几乎和“所有人类最终都会死亡”一样可靠。到目前为止，99.9%的物种已经从平衡木上跌落，似乎很明显，如果一个物种继续在平衡木上摇摆，总有一天会被其他物种、一阵自然风、或突然震动平衡木的小行星击倒。Bostrom称灭绝为*吸引态*——一个所有物种都在摇摇欲坠、从中跌落并且没有物种能够返回的地方。

虽然我遇到的大多数科学家都承认，ASI有能力让人类灭绝，但许多人也相信，合理使用ASI的能力可以使个人和整个物种达到*第二个*吸引态——物种不朽。Bostrom认为物种不朽和物种灭绝一样是一个吸引态，即如果我们能到达那里，我们将永远不受灭绝的影响——我们将征服死亡和征服偶然性。因此，尽管*到目前为止*所有物种都已经从平衡木上跌落并陷入灭绝，Bostrom认为，平衡木有两边，只是地球上还没有任何东西足够聪明，找到如何跌落到另一边的方法。

![beam2](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/beam2.jpg)

如果Bostrom和其他人是对的，并且从我读到的一切来看，他们似乎真的可能是对的，我们有两个相当令人震惊的事实需要吸收：

**1）ASI的出现将首次打开一个物种落在平衡木不朽一侧的可能性。**

**2）ASI的出现将产生如此难以想象的巨大影响，它很可能会将人类从平衡木上击倒，无论是向哪个方向。**

当进化触发引线时，它可能会永久结束人类与平衡木的关系，并创造一个有或没有人类的新世界。

似乎每个人类目前应该问的唯一问题是： *我们什么时候会触发引线，当发生时我们会落在平衡木的哪一边？*

世界上没有人知道这个问题的任何部分的答案，但许多最聪明的人已经为此思考了几十年。我们将在本文的剩余部分探讨他们的结论。

让我们从问题的第一个部分开始：

## 我们什么时候会触发引线？

也就是说，距离第一台机器达到超级智能还有多久？

毫不奇怪，意见大相径庭，这是科学家和思想家之间的一个激烈辩论。许多人，比如教授[Vernor Vinge](https://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html)、科学家[Ben Goertzel](http://goertzel.org/TenYearsToTheSingularity.pdf)、Sun Microsystems的联合创始人[Bill Joy](http://archive.wired.com/wired/archive/8.04/joy.html)，或者最著名的发明家和未来学家[Ray Kurzweil](https://en.wikipedia.org/wiki/Predictions_made_by_Ray_Kurzweil)，他们都同意机器学习专家Jeremy Howard在一次[TED演讲](http://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn?language=en)中展示的这张图：

![Howard Graph](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Howard-Graph.png)

这些人认为，这一切*即将发生*——指数增长正在发挥作用，而机器学习虽然目前缓慢地逼近我们，但在未来几十年内会迅速超过我们。

其他人，比如微软的联合创始人[Paul Allen](http://www.technologyreview.com/view/425733/paul-allen-the-singularity-isnt-near/)、研究心理学家[Gary Marcus](http://www.newyorker.com/tech/elements/hyping-artificial-intelligence-yet-again)、纽约大学计算机科学家[Ernest Davis](http://www.aaai.org/ojs/index.php/aimagazine/article/view/568)和科技企业家[Mitch Kapor](http://longbets.org/1/)，认为像Kurzweil这样的思想家[极大低估了](http://www.technologyreview.com/view/425733/paul-allen-the-singularity-isnt-near/)这一挑战的难度，认为我们实际上离触发引线还很远。

Kurzweil阵营会[反驳](http://www.technologyreview.com/view/425818/kurzweil-responds-dont-underestimate-the-singularity/)，说唯一的低估是在于对指数增长的低估，他们会把质疑者比作那些在1985年看到互联网缓慢发展的幼苗，认为互联网在不久的将来不会有什么影响的人。

质疑者可能会反驳说，提升智能所需的进展也会随着每一步骤而成倍增加，这将抵消技术进步的典型指数增长。如此反复争论。

第三种观点，包括[Nick Bostrom](https://www.amazon.com/gp/product/0199678111/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0199678111&linkCode=as2&tag=wabuwh00-20&linkId=LBOTX2G2R72P5EUA)在内，既不认为任何一方有足够的依据对时间线充满信心，同时也承认A）这绝对可能在不久的将来发生，B）没有保证这一定会发生；也可能需要更长时间。

还有一些人，比如哲学家[Hubert Dreyfus](https://www.amazon.com/gp/product/0262540673/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0262540673&linkCode=as2&tag=wabuwh00-20&linkId=ZHBAVUQOM6SIGYHG)，认为这三组人都太天真了，认为根本不存在所谓的引线，超级智能实际上可能永远不会实现。

那么，当所有这些观点汇总在一起时会得到什么？

2013年，Vincent C. Müller和Nick Bostrom进行了一项调查，在一系列会议上询问数百名AI专家以下问题：“假设人类科学活动持续进行，没有重大负面干扰。你认为在什么年份（10% / 50% / 90%）的概率会存在这样的高水平机器智能[^4b]？” 调查要求他们给出一个乐观年份（他们认为有10%概率会有AGI的年份），一个现实猜测（他们认为有50%概率会有AGI的年份——即在那之后他们认为我们更有可能拥有AGI），以及一个安全猜测（他们可以说90%确定会有AGI的最早年份）。汇总成一个数据集后，结果如下：[^2g]

中位数乐观年份（10%可能性）：**2022年**  
中位数现实年份（50%可能性）：**2040年**  
中位数悲观年份（90%可能性）：**2075年**  

因此，中位参与者认为，我们很可能在25年后拥有AGI。2075年的90%中位数答案意味着，如果你现在是青少年，调查的中位数受访者和超过一半的AI专家几乎确定AGI将在你的一生中实现。

在最近由作者James Barrat在Ben Goertzel的年度AGI会议上进行的另一项研究中，放弃了百分比，直接询问参与者认为AGI会在何时实现——2030年前、2050年前、2100年前、2100年后或从不。结果如下：[^3g]

2030年前：**42%的受访者**  
2050年前：**25%**  
2100年前：**20%**  
2100年后：**10%**  
从不：**2%**

结果与Müller和Bostrom的结果相似。在Barrat的调查中，超过三分之二的参与者认为AGI将在2050年前出现，略少于一半的人预测AGI将在未来15年内出现。值得注意的是，只有2%的受访者认为AGI不是我们未来的一部分。

但AGI不是引线，ASI才是。那么专家们认为我们何时会达到ASI？

Müller和Bostrom还询问了专家们，他们认为我们在达到AGI后A）在两年内达到ASI的可能性（即几乎立即的智能爆炸），以及B）在30年内达到ASI的可能性。结果如下：[^4g]

中位数答案认为快速（2年）从AGI过渡到ASI的可能性只有**10%**，但30年内过渡的可能性为**75%**。

我们从这些数据中无法得知中位数参与者会给出多少年的过渡期为50%的可能性，但根据上述两个答案，估计他们会说是20年。因此，中位数意见——即AI专家界的中间意见——认为达到ASI引线的最现实猜测是\[2040年预测的AGI + 我们估计的从AGI到ASI的20年过渡\] **\= 2060年**。

![Timeline](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Timeline.png)

当然，上述所有统计数据都是推测性的，它们只是AI专家社区中心意见的代表，但它告诉我们，大部分最了解这一主题的人会同意，2060年是潜在改变世界的ASI到来的一个非常合理的估计。距今只有45年。

好，现在来看问题的第二部分： *当我们触发引线时，我们会跌落到平衡木的哪一边？*

超级智能将带来巨大的力量——对我们来说至关重要的问题是：

谁或什么将掌控这种力量，他们的动机是什么？

这个问题的答案将决定ASI是一个令人难以置信的伟大发展，还是一个无法想象的可怕发展，或者介于两者之间。

当然，专家社区对这个问题的答案也各不相同，并且激烈辩论。Müller和Bostrom的调查要求参与者对AGI对人类的可能影响进行概率分配，发现平均回答是**52%认为结果会是好或非常好，31%认为结果会是坏或非常坏。** 对于相对中立的结果，平均概率只有17%。换句话说，那些最了解这一主题的人很确定这将是一个重大事件。值得注意的是，这些数字指的是*AGI*的到来——如果问题是关于ASI，我想中立的百分比会更低。

在深入探讨这个好与坏的结果问题之前，让我们将“何时发生？”和“结果是好是坏？”这两个问题结合成一个图表，涵盖大多数相关专家的观点：

![Square1](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Square11.jpg)

我们稍后会详细讨论主流阵营，但首先——你现在的看法是什么？实际上我知道你现在的看法，因为在我开始研究这个话题之前，我也是这样的。大多数人没有认真思考这个话题的原因：

*   正如在第一部分中提到的，电影通过呈现不现实的AI情景让我们觉得 AI总体上不值得认真对待。James Barrat将这种情况比作如果疾病控制中心发布了关于未来吸血鬼的严重警告，我们的反应。[^5g]
*   人类很难相信某件事是真的，直到我们看到证据。我相信1988年的计算机科学家经常谈论互联网可能会有多大的影响，但人们可能并不*真正*认为这会改变他们的生活，直到它实际改变了他们的生活。这部分是因为1988年的计算机根本做不到那样的事情，所以人们会看着他们的计算机，想，“真的？*那*会是一个改变生活的东西？” 他们的想象力受限于个人经验教给他们的计算机是什么，这使得他们很难生动地想象计算机可能*变成什么*。现在对AI也发生了同样的事情。我们听说这将是一个大事，但因为它还没有发生，并且因为我们对当前世界中相对无力的AI的经验，我们很难*真正*相信这会大幅改变我们的生活。这些偏见就是专家们在努力通过我们集体的日常自我沉迷的噪音引起我们注意时所面临的挑战。
*   即使我们相信了——你今天有多少次想到你将度过大部分永恒的时间而不存在？不多吧？即使这是一个比你今天做的任何事情都更为强烈的事实？这是因为我们的头脑通常专注于日常生活中的小事，无论我们处于*多么疯狂*的长期情境中。这只是我们的思维方式。

这两篇文章的目标之一是让你从我喜欢思考其他事情的阵营转移到专家阵营，即使你只是站在上图虚线的交汇处，完全不确定。

在我的研究中，我遇到了许多不同的意见，但我很快注意到，大多数人的意见都落在我标记的主流阵营内，尤其是超过四分之三的专家落入主流阵营内的两个子阵营中：

![Square2](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Square21.jpg)

我们将深入探讨这两个阵营。让我们先从有趣的那个开始——

## 为什么未来可能是我们最大的梦想

在我了解AI的世界时，我发现有相当多的人站在这里：

![Square3](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Square32.jpg)

“自信角”的人们兴奋不已。他们把目光投向了平衡木的欢乐一侧，并且确信我们所有人都会朝那个方向前进。对他们来说，未来是他们梦寐以求的一切，而且正好赶上。

将这些人与我们稍后讨论的其他思想家区分开来的不是他们对欢乐一侧的渴望，而是他们对我们将会落在那一侧的信心。

这种信心来自哪里还存在争议。批评者认为，这种信心源于一种如此令人眼花缭乱的兴奋，以至于他们简单地忽视或否认了潜在的负面结果。但信奉者认为，在总体上，技术已经并且可能会继续对我们有更多帮助，而不是伤害我们，因此召唤世界末日场景是天真的。

我们将覆盖两种观点，你可以在阅读时形成自己的意见，但在这一部分，请暂时放下怀疑，认真看看平衡木的欢乐一侧有什么——并试着吸收你正在阅读的事情*可能真的会发生*。如果你向一个狩猎采集者展示我们这个充满室内舒适、科技和无尽丰饶的世界，对他来说，这看起来像虚构的魔法——我们必须谦卑地承认，*可能*在我们的未来也会有同样难以置信的转变。

Nick Bostrom描述了超级智能AI系统可以发挥作用的三种方式：[^6g]

*   作为一个**神谕者**，准确回答几乎任何问题，包括人类无法轻易回答的复杂问题——即*如何制造一个更高效的汽车引擎？*Google是一个原始类型的神谕者。
*   作为一个**精灵**，执行任何给定的高层次指令——*使用分子装配机制造一种新的、更高效的汽车引擎*——然后等待下一个命令。
*   作为一个**主权者**，被赋予一个广泛和开放的追求，并允许在世界上自由行动，自行决定如何最好地进行——*发明一种比汽车更快、更便宜、更安全的私人运输方式*。

这些对我们来说看似复杂的问题，对一个超级智能系统来说，就像有人要求你改进“我的铅笔掉在桌子上”这种情况一样，你会通过捡起它并放回桌子上来解决。

[Eliezer Yudkowsky](http://www.yudkowsky.net/)，我们上图中“焦虑大道”的居民，说得很好：

> 没有难题，只有对某种智力水平来说难的问题。\[智力水平\]稍微提高一点，一些问题会突然从“不可解决”变得“显而易见”。提高到一定程度，所有问题都会变得显而易见。[^7g]

在“自信角”有很多热情的科学家、发明家和企业家——但要想了解AI地平线上最光明的一面，只有一个人适合做我们的向导。

Ray Kurzweil是个两极分化的人物。在我的阅读中，我听到了对他和他思想的神一样的崇拜，也听到了对他的翻白眼轻视。其他人则处于中间地带——作家Douglas Hofstadter在讨论Kurzweil书中的观点时，优雅地提出，“这就像你把很多非常好的食物和一些狗屎混合在一起，这样你就根本无法弄清楚什么是好的，什么是坏的。”[^8g]

不管你是否喜欢他的观点，每个人都同意Kurzweil是令人印象深刻的。他在青少年时期就开始发明东西，随后几十年，他发明了几项突破性发明，包括第一个平板扫描仪，第一个将文本转换为语音的扫描仪（让盲人可以阅读标准文本），著名的Kurzweil音乐合成器（第一个真正的电子钢琴），以及第一个商业化的大词汇量语音识别系统。他是五本全国畅销书的作者。他以大胆的预测而闻名，并且有一个[非常不错的记录](http://bigthink.com/endless-innovation/why-ray-kurzweils-predictions-are-right-86-of-the-time)，这些预测后来都成真——包括他在80年代末的预测，当时互联网还是一个不为人知的事物，到2000年代初它将成为一种全球现象。Kurzweil被《华尔街日报》称为“永不止息的天才”，被《福布斯》称为“终极思维机器”，被《Inc.》杂志称为“爱迪生的合法继承人”，被比尔·盖茨称为“我所知最能预测人工智能未来的人”。[^9g] 2012年，Google联合创始人Larry Page邀请Kurzweil担任Google的工程总监。[^5b] 2011年，他与人共同创办了[奇点大学](https://su.org/)，由NASA主办，并部分由Google赞助。对于一个人来说，这已经很不错了。

这个传记很重要。当Kurzweil阐述他对未来的愿景时，他听起来完全像一个疯子，而疯狂的是，他不是——他是一个极其聪明、知识渊博、在世界上有影响力的人。你可能认为他对未来的预测是错的，但他不是傻子。知道他是这样一个靠谱的人让我感到*开心*，因为当我了解到他的未来预测时，我*非常*希望他是对的。你也会这么想。当你听到Kurzweil的预测，许多像[Peter Diamandis](http://www.diamandis.com/)和[Ben Goertzel](https://en.wikipedia.org/wiki/Ben_Goertzel)这样的“自信角”思想家分享的观点时，很容易看出为什么他有如此庞大、热情的追随者——被称为奇点主义者。以下是他认为将会发生的事情：

### 时间表

Kurzweil认为，计算机将在2029年达到AGI，到2045年，我们不仅会有ASI，还会有一个全面的新世界——他称之为奇点的时代。他与AI相关的时间表曾被认为是极度过于乐观的，很多人仍然这么认为，[^6b] 但在过去15年中，ANI系统的快速进步使得更多的AI专家逐渐接近Kurzweil的时间表。尽管他的预测仍比Müller和Bostrom调查的中位数受访者更为雄心勃勃（AGI在2040年，ASI在2060年），但差距并不大。

Kurzweil描绘的2045年奇点是通过三大革命同时发生的——生物技术、纳米技术和最强大的人工智能。

在我们继续之前——纳米技术几乎出现在你读到的每一个关于AI的未来讨论中，所以请进入这个蓝色框，我们可以讨论一下——

### 纳米技术蓝盒

纳米技术是我们处理1到100纳米之间的物质的技术。一个纳米是十亿分之一米，或百万分之一毫米，这个1-100纳米范围包括病毒（100纳米宽）、DNA（10纳米宽），以及像血红蛋白（5纳米）和葡萄糖（1纳米）这样的分子。如果/当我们征服纳米技术时，下一步将是操纵个别原子，它们只有一个数量级更小（约0.1纳米）。[^7b]

要理解人类试图操纵这一范围内物质的挑战，让我们从更大的规模来看同样的事情。国际空间站距离地球268英里（431公里）。如果人类是如此巨大的巨人，他们的头部可以到达国际空间站，他们将比现在大约大25万倍。如果将1纳米到100纳米的纳米技术范围放大25万倍，就得到0.25毫米到2.5厘米。所以纳米技术相当于一个像国际空间站那么高的人类巨人，试图用沙粒到眼球大小的材料仔细建造复杂的物体。要达到下一个水平——操纵单个原子——巨人需要仔细定位1/40毫米大小的物体——如此之小，正常大小的人类需要显微镜才能看到它们。[^8b]

纳米技术首次由Richard Feynman在1959年的一次演讲中讨论，当时他解释说：“据我所知，物理学原理并不反对原子操纵的可能性。原则上，物理学家可以合成化学家写下的任何化学物质。如何？把原子放在化学家说的地方，这样你就能制造出这种物质。”就是这么简单。如果你能弄清楚如何移动单个分子或原子，你就可以制造出任何东西。

纳米技术在1986年首次成为一个严肃的领域，当时工程师Eric Drexler在他的重要著作《创造的引擎》中奠定了其基础，但Drexler建议，那些希望了解最现代纳米技术思想的人最好阅读他2013年的著作[*Radical Abundance*](https://www.amazon.com/gp/product/1610391136/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=1610391136&linkCode=as2&tag=wabuwh00-20&linkId=AIDAWXXXQCHLBDCU)。

### 灰史莱姆蓝盒

我们现在在一个*分支中的分支*。这非常有趣。[^9b]

无论如何，我把你带到这里是因为我需要告诉你一个关于纳米技术传说中的非常不好笑的部分。在早期版本的纳米技术理论中，有一种提议的方法是通过创建数万亿个微小的纳米机器人来共同建造某物。创建数万亿个纳米机器人的一种方法是制造一个可以自我复制的，然后让它通过复制过程将一个变成两个，这两个再变成四个，四个变成八个，大约一天之内，就会有几万亿个准备就绪。这就是指数增长的力量。聪明吧？

这很聪明，直到它意外地引发全球性的地球毁灭性灾难。问题在于，指数增长的同样力量使得快速创建数万亿纳米机器人变得超级方便的同时，也使得自我复制成为一个*可怕*的前景。因为如果系统出故障，不是在预期的数量达到几万亿时停止复制，而是继续复制呢？这些纳米机器人会被设计成消耗任何碳基材料来维持复制过程，而不幸的是，所有生命都是碳基的。地球的生物量大约含有1045个碳原子。一个纳米机器人大约由106个碳原子组成，所以1039个纳米机器人就会消耗掉地球上的所有生命，这将在130次复制（2130大约等于1039）中发生，因为纳米机器人大军（即灰史莱姆）会在地球上蔓延。科学家们认为纳米机器人可以在大约100秒内复制一次，这意味着这个简单的错误将在3.5小时内不方便地终结地球上的所有生命。

更糟糕的情景是——如果一个恐怖分子得到了纳米机器人技术并且知道如何编程，他可以制造出最初的几万亿个纳米机器人，并编程让它们在几周内悄悄地分布在世界各地而不被发现。然后，它们会同时发动攻击，只需90分钟就能消耗掉所有东西——而且由于它们分布在各处，根本无法对抗它们。[^10b]

尽管这个恐怖故事已经被广泛讨论了很多年，好消息是它可能被夸大了——创造了“灰史莱姆”这个术语的Eric Drexler在这篇文章之后给我发了一封电子邮件，表达了他对灰史莱姆场景的看法：“人们喜欢恐怖故事，这个故事属于僵尸题材。这个想法本身就像在吃脑子一样。”

一旦我们真正掌握了纳米技术，我们可以用它制造科技设备、衣物、食物、各种生物相关产品——人工血细胞、微小的病毒或癌细胞摧毁者、肌肉组织等——几乎任何东西。在一个使用纳米技术的世界中，材料的成本不再与其稀缺性或制造过程的难度有关，而是由其原子结构的复杂程度决定。在纳米技术世界中，钻石可能比铅笔橡皮擦还便宜。

我们还没有到达那个阶段。而且目前还不清楚我们是否低估了或高估了达到那个阶段的难度。但我们似乎离得并不远。Kurzweil预测我们将在2020年代达到那个阶段。[^11b] 政府知道纳米技术可能是一个颠覆性的发现，因此已经在纳米技术研究上投入了数十亿美元（美国、欧盟和日本已经联合投资超过50亿美元）。[^12b]

仅仅考虑一下如果一个超级智能计算机拥有一个强大的纳米级装配器的可能性就已经让人感到强烈。但纳米技术是*我们*发明的，我们即将征服的东西，由于我们能做到的任何事情对ASI系统来说都是小菜一碟，我们必须假设ASI会发明出更强大的技术，远远超出人类大脑的理解能力。因此，当考虑“如果AI革命对我们有利”这个情景时，我们几乎不可能高估可能发生的事情的范围——所以如果以下对ASI未来的预测看起来过于夸张，请记住，这些可能会以我们无法想象的方式实现。最有可能的是，我们的大脑甚至无法预测将会发生的事情。

### AI能为我们做什么

![only-humans-cartoon](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/only-humans-cartoon.jpg)

[来源](https://www.amazon.com/gp/product/0143037889/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0143037889&linkCode=as2&tag=wabuwh00-20&linkId=54Q62R5PYJBEENTP)

拥有超级智能和所有超级智能会知道如何创造的技术，ASI可能会解决人类的每一个问题。全球变暖？ASI可以首先通过发明更好的发电方法来停止二氧化碳排放，这些方法与化石燃料无关。然后，它可以创造一些创新的方法开始从大气中去除多余的二氧化碳。癌症和其他疾病？对ASI来说没有问题——健康和医学将会被革命性地改变。世界饥饿？ASI可以利用纳米技术从头*制造*与真正的肉分子结构完全相同的肉——换句话说，它会*成为*真正的肉。纳米技术可以将一堆垃圾变成一大桶新鲜的肉或其他食物（这些食物不必有正常的形状——可以想象一个巨大的苹果方块）——并使用超先进的运输方式将这些食物分发到世界各地。当然，这对动物来说也很好，因为它们不再需要被人类杀害，ASI还可以做很多其他事情来拯救濒危物种，甚至通过研究保存的DNA使灭绝的物种复活。ASI甚至可以解决我们最复杂的宏观问题——我们的经济运行方式和世界贸易最佳方式的争论，甚至是我们在哲学或伦理学方面最模糊的困惑——对ASI来说，这些都显而易见。

但有一件事是ASI能为我们做的，是*如此*诱人，读到它改变了我对一切的看法：

***ASI可以让我们征服我们的死亡。***

几个月前，我[提到过](https://waitbutwhy.com/2014/05/fermi-paradox.html)我对那些已经征服了自身死亡的更先进潜在文明的羡慕，从未考虑过我可能会在以后写一篇文章，让我真正相信这是人类在我有生之年可以做到的事情。但读关于AI的文章会让你重新考虑*所有*你以为自己确定的事情——包括你对死亡的看法。

进化没有充分的理由延长我们的寿命。如果我们活得足够长以繁殖并养育我们的孩子直到他们能够自力更生，从进化的角度来看，这对物种繁荣来说就足够了——因此，没有理由认为自然选择过程中会有有利于异常长寿的突变。结果就是，我们成了W.B. Yeats所描述的“灵魂被固定在一个垂死的动物上”。[^13g] 这并不有趣。

因为每个人一直都会死，我们生活在“死亡和税收”的假设之下，认为死亡是不可避免的。我们把衰老看作是时间——两者都在继续前进，我们无法阻止它们。*但*这种假设是错误的。理查德·费曼写道：

> 在所有生物科学中，最引人注目的事情之一是，没有任何迹象表明死亡是必然的。如果你说我们想要制造永动机，当我们研究物理学时，我们发现了足够多的定律，看到这是绝对不可能的，除非这些定律是错误的。但在生物学中，还没有发现任何表明死亡是必然的东西。这让我觉得，死亡并非必然，只是时间问题，生物学家会发现是什么原因导致了我们的困扰，这种可怕的普遍性疾病或人类身体的暂时性将会被治愈。

事实是，衰老并不*依附于*时间。时间*会*继续前进，但*衰老不必*。如果你仔细想想，这很有道理。所有衰老只是身体的物质在磨损。汽车也会随着时间的推移而磨损——但它的衰老是不可避免的吗？如果你在每一个部件开始磨损时完美地修复或更换它，汽车就会永远运行。人体也不例外——只是复杂得多。

库兹韦尔谈到智能的、连接WiFi的纳米机器人可以进入血液中，为人类健康执行无数任务，包括定期修复或更换身体任何部位的磨损细胞。如果这个过程（或者ASI会想到的更聪明的方法）得以完善，不仅能保持身体健康，还能*逆转衰老*。一个60岁身体与30岁身体的区别只是一些物理上的东西，如果我们拥有技术，这些都可以改变。ASI可以构建一个“年龄刷新器”，一个60岁的人可以走进去，出来时拥有30岁的身体和皮肤。[^10g]即使是一直令人困惑的大脑，也可以通过像ASI那样聪明的东西进行刷新，而不会影响大脑的数据（个性、记忆等）。一个患有痴呆症的90岁老人可以进入年龄刷新器，出来时头脑敏锐，准备开始一份全新的职业。这看起来很荒谬——但身体只是一堆原子，而ASI可以轻松操纵各种原子结构——所以这*并不荒谬*。

库兹韦尔然后又迈出了一大步。他认为人工材料将随着时间的推移越来越多地集成到人体中。首先，器官可以被超级先进的机器版本所取代，这些机器版本将永远运行且永不失效。然后他认为我们可以开始重新设计身体——例如用完美的红细胞纳米机器人取代红细胞，这些纳米机器人可以自己供能，完全消除了对心脏的需求。他甚至谈到了大脑，认为我们将增强大脑活动到一个人类能够*思考*的速度是现在的数十亿倍，并能够访问外部信息，因为人工添加到大脑中的部分将能够与云端的所有信息通信。

新的体验可能性将是无穷无尽的。人类已经将性与其目的分开，允许人们为了乐趣而进行性行为，而不仅仅是为了生殖。库兹韦尔相信我们可以对食物做同样的事情。纳米机器人将负责向身体细胞提供完美的营养，智能地引导任何不健康的东西通过身体而不影响任何东西。一个饮食避孕套。纳米技术理论家Robert A. Freitas已经设计了红细胞替代品，如果有一天被植入体内，可以让一个人不用呼吸跑15分钟——所以你只能想象ASI能为我们的身体能力做些什么。虚拟现实将有新的意义——体内的纳米机器人可以抑制来自我们感官的输入，并用新的信号替代它们，让我们完全进入一个新的环境，一个我们可以看到、听到、感觉到和闻到的环境。

最终，库兹韦尔认为人类将达到一个完全人工的阶段；[^11g] 那时我们将看着生物材料，认为人类曾由*那种东西*构成是多么不可思议地原始；那时我们将阅读人类历史的早期阶段，当时微生物、事故、疾病或磨损可以违背人类意愿杀死人类；AI革命可以通过人类与AI的*融合*来结束这一切。[^12g] 库兹韦尔认为这就是人类最终征服我们的生物学并变得坚不可摧和永恒的方式——这是他对平衡木另一侧的愿景。而且他相信我们会到达那里。很快。

你不会感到惊讶地发现库兹韦尔的想法引起了不少批评。他预测2045年奇点将到来以及随后人类永生的可能性被嘲笑为“书呆子的狂热”，或者“140智商人群的智能设计”。其他人质疑他乐观的时间表，或者他对大脑和身体的理解水平，或者他将通常应用于硬件进步的摩尔定律模式应用于包括软件在内的广泛事物的做法。对于每一个坚信库兹韦尔正确的专家，可能有三个认为他大错特错。

但让我惊讶的是，大多数不同意他的专家并不是真的不同意他说的一切都可能发生。读到这样一个离奇的未来愿景时，我本以为他的批评者会说，“显然这些事情不可能发生”，但他们却在说，“是的，如果我们安全过渡到ASI，这一切都可能发生，但这是最难的部分。”博斯特罗姆，警告我们AI危险的最著名声音之一，仍然承认：

> 很难想象有任何问题是超级智能无法解决或至少帮助我们解决的。疾病、贫困、环境破坏、各种不必要的痛苦：这些都是装备了先进纳米技术的超级智能能够消除的。此外，超级智能可以给予我们无限的寿命，无论是通过使用纳米医学停止和逆转衰老过程，还是通过为我们提供上传自己的选项。超级智能还可以为我们创造机会，大幅提高我们的智力和情感能力，并帮助我们创造一个高度吸引人的体验世界，在那里我们可以过上致力于快乐游戏、彼此交流、体验、个人成长和更接近我们理想的生活。

这是一个非常*不*在自信角的人所说的话，但这就是我不断发现的——专家们嘲笑库兹韦尔出于各种原因，但他们*不*认为他说的一切是不可能的*如果*我们能安全过渡到ASI。这就是为什么我发现库兹韦尔的想法如此有感染力——因为它们阐明了这个故事的光明面，而且它们实际上是可能的。*如果它是一个好上帝。*

我听到的对自信角思想家的最突出的批评是他们在评估ASI带来的负面影响时可能*危险地错误*。库兹韦尔那本著名的书《奇点临近》有700多页，他只花了大约20页讨论潜在的危险。我之前提到过，我们的命运在这种巨大的新力量诞生时取决于谁将控制这种力量以及他们的动机是什么。库兹韦尔用一句话简洁地回答了这两个问题，“\[ASI\] 是从许多不同的努力中涌现出来的，并将深深地融入到我们文明的基础设施中。确实，它将密切嵌入我们的身体和大脑。因此，它将反映我们的价值观，因为它将是我们。”

但如果这是答案，为什么现在世界上这么多最聪明的人如此担忧？为什么斯蒂芬·霍金[说](http://www.washingtonpost.com/news/speaking-of-science/wp/2014/12/02/stephen-hawking-just-got-an-artificial-intelligence-upgrade-but-still-thinks-it-could-bring-an-end-to-mankind/) 发展ASI“可能意味着人类的终结”，比尔·盖茨[说](http://www.washingtonpost.com/blogs/the-switch/wp/2015/01/28/bill-gates-on-dangers-of-artificial-intelligence-dont-understand-why-some-people-are-not-concerned/) 他不“明白为什么有些人不担心”，埃隆·马斯克[担心](https://www.theguardian.com/technology/2014/oct/27/elon-musk-artificial-intelligence-ai-biggest-existential-threat) 我们正在“召唤恶魔”？为什么那么多专家称ASI是对人类最大的威胁？这些人，以及其他在焦虑大道上的思想家，不相信库兹韦尔对AI危险的轻描淡写。他们非常，非常担心AI革命，并且他们并没有关注平衡木的乐趣一侧。他们忙于盯着另一边，那里他们看到了一个可怕的未来，一个他们不确定我们是否能够逃脱的未来。

\_\_\_\_\_\_\_\_\_\_\_

## 为什么未来可能是我们的最坏噩梦

我想了解人工智能的原因之一是，“坏机器人”这个话题总是让我感到困惑。所有关于邪恶机器人的电影似乎都是不切实际的，我无法真正理解为什么会有真正的生活情况下人工智能真的会危险。机器人是由*我们*制造的，所以为什么我们会以某种方式设计它们，以致于会出现负面情况？我们不是会内置足够多的安全防护措施吗？我们不是随时都可以切断人工智能系统的电源并将其关闭吗？为什么一个机器人会想做一些坏事？一个机器人为什么“想”*什么*？我非常怀疑。但是接着我听到了一些非常聪明的人在谈论…

这些人往往在这里的某个地方：

![Square4](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Square42.jpg)

焦虑大道上的人们并不在恐慌的草原或绝望的山丘上——这两个地区都在图表的*最*左侧——但他们很紧张，很紧张。位于图表中间并不意味着你认为ASI的到来将是*中性的——*中立者有自己的阵营——而是意味着你认为极端好和极端坏的结果都有可能，但你还不确定到底会是哪一个。

所有这些人中的一部分都对人工超级智能可能为我们做些什么充满了激动——只是他们有点担心这可能是*失落方舟*的开始，而人类就是这个家伙：

![raiders](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/raiders1.jpg)

他站在那里，拿着鞭子和他的偶像，以为自己已经完全搞清楚了一切，当他说出他的“再见，先生”时，他对自己感到非常高兴，然后突然间他不那么高兴了，因为发生了这种事情。

![500px-Satipo_death](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/500px-Satipo_death.jpg)

（抱歉）

与此同时，印第安纳·琼斯，他更加了解和谨慎，理解危险以及如何避免危险，安全地走出了洞穴。当我听到焦虑大道上的人们对人工智能说些什么时，它经常听起来像是在说：“嗯，我们现在有点像是第一个家伙，但相反我们应该努力尽量成为印第安纳·琼斯。”

那么到底是什么让焦虑大道上的每个人都感到不安呢？

首先，从广义上讲，当我们开发超级智能人工智能时，我们正在创造一种可能会改变一切的东西，但却是在完全未知的领域，我们不知道到达那里后会发生什么。科学家丹尼·希利斯将正在发生的事情比作那个时候“单细胞生物正在转变为多细胞生物的时刻。我们是变形虫，我们无法弄清楚我们正在创造的到底是什么。”[^14g] 尼克·博斯特罗姆担心，创造出比自己更聪明的东西是一种基本的达尔文错误，并将这种兴奋与一些麻雀在巢中决定收养一只小猫头鹰，以便它在长大后能帮助他们并保护他们进行了比喻——而忽略了一些麻雀的迫切呼声，他们怀疑这是否真的是一个好主意…[^15g]

当你将“未知、不太了解的领域”与“一旦发生将产生重大影响”的情况相结合，你就会打开英语中最可怕的两个词的大门：

*生存威胁。*

生存威胁是指可能对人类造成永久性灾难性影响的事物。通常，生存威胁意味着灭绝。看看博斯特罗姆的一个[Google演讲](https://www.youtube.com/watch?v=pywF6ZzsghI)中的这个图表：[^13b]

![Existential Risk Chart](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Existential-Risk-Chart.jpg)

你可以看到“生存威胁”的标签是保留给跨越物种、代代相传的事物（即永久性的），并且在其后果方面具有毁灭性或致命性的事物。[^14b]它技术上包括一种情况，即所有人类永远处于苦难或折磨之中，但再次强调，我们通常谈论的是灭绝。能够对人类造成灭绝性灾难的有三种情况：

**1) 自然**——大型小行星碰撞、大气变化使空气对人类不适宜、席卷世界的致命病毒或细菌性疾病等。

**2) 外星人**——这是史蒂芬·霍金、卡尔·萨根和许多其他天文学家在劝告METI停止发出外向信号时[害怕的事情](https://waitbutwhy.com/2014/05/fermi-paradox.html)。他们不希望我们成为印第安人，并让所有潜在的欧洲征服者知道我们的存在。

**3) 人类**——拥有能够导致灭绝的武器的恐怖分子，全球范围内的灾难性战争，人类在不仔细考虑的情况下创建比自己更聪明的东西…

博斯特罗姆指出，如果前两种情况在我们作为一个物种的前10万年里还没有灭绝我们，那么在未来一个世纪发生的可能性是不大的。

然而，第三种情况让他感到恐惧。他用一个装有许多弹珠的罐子作了一个隐喻。假设大多数弹珠是白色的，少数是红色的，极少数是黑色的。每次人类发明新东西，就像从罐子中拿出一个弹珠一样。大多数发明对人类是中立的或有益的——这些是白色弹珠。有些对人类是有害的，比如大规模杀伤性武器，但它们不会造成灭绝——红色弹珠。如果我们曾经发明了一种会导致我们灭绝的东西，那就是拿出了罕见的黑色弹珠。我们还没有拿出黑色弹珠——你知道这一点，因为你活着，正在阅读这篇文章。但博斯特罗姆认为，在不久的将来我们很可能会拿出一个。如果核武器，例如，变得*容易*制造而不是极其困难和复杂，恐怖分子早就会把人类炸回石器时代了。核武器并不是一个黑色弹珠*但它们离黑色弹珠并不远。*博斯特罗姆认为，ASI是迄今为止我们最强大的黑色弹珠候选者。[^15b]

因此，你会听到关于ASI可能带来的许多负面事情——随着人工智能接管越来越多的工作，失业率飙升，[^16b]如果我们设法解决了衰老问题，人口膨胀，[^17b]等等。但我们唯一应该过分关注的是*大问题*: 灭绝的前景。

这让我们回到了本文开头的关键问题：**当ASI到来时，谁或什么将掌握这种巨大的新权力，他们的动机将是什么？**

在涉及到哪些代理-动机组合会很糟糕时，有两个很快就会想到：一个恶意的人类/一群恶意的人类/政府，和一个恶意的ASI。那么这两种会是什么样子呢？

**一个恶意的人类、一群恶意的人类或政府开发了第一个ASI，并使用它执行他们的邪恶计划。** 我称之为贾法尔情景，就像贾法尔得到了魔灯并对此感到烦恼和专制一样。所以——如果ISIS有几个天才工程师在他们的麾下，正在为AI开发而疯狂工作呢？或者如果伊朗或朝鲜在某种程度上幸运地对AI系统进行了关键调整，使它在接下来的一年里迅速提升到ASI水平呢？这肯定是不好的——但在这些情景中，大多数专家都不担心ASI的人类创造者会用他们的ASI做坏事，他们担心的是创作者们会*匆忙*制造第一个ASI，并且在没有仔细考虑的情况下这样做，因此他们会失去对它的控制。然后那个ASI系统的动机将决定那些创造者以及其他人的命运。专家们确实认为恶意的人类代理人可能会借助一个为其工作的ASI造成可怕的破坏，但他们似乎不认为这种情况是杀死我们所有人的可能情况，因为他们相信坏人会和好人一样无法控制一个ASI。好，那么—

**一个恶意的ASI被创建，并决定摧毁我们所有人。** 每一部AI电影的情节。人工智能变得和人类一样聪明，然后决定反对我们并接管控制权。这是我希望你在本文剩余部分中能够清楚的一点：*没有任何警告我们AI问题的人在讨论这个情况。*邪恶是一个人类的概念，将人类的概念应用于非人类的事物称为“拟人化”。避免拟人化的挑战将是本文剩余部分的一个主题。没有任何AI系统会像电影中描述的那样*变得邪恶*。

### AI意识蓝盒

这也触及了与AI相关的另一个重大话题——*意识*。如果一个AI变得足够聪明，它将能够和我们一起笑，和我们一起挖苦，它会声称和我们一样有情感，但它真的会*有*那些感觉吗？它只会*看起来有*自我意识还是*真的有*自我意识？换句话说，一个聪明的AI*是否真的有*意识，还是只是 *表现出*有意识？

对于这个问题进行了深入的探讨，引发了许多[争论](http://www.kurzweilai.net/gelernter-kurzweil-debate-machine-consciousness-2)，以及像约翰·西尔的[中国屋](https://en.wikipedia.org/wiki/Chinese_room)这样的思想实验（他用它来表明没有计算机能够拥有意识）。这是一个重要的问题，有许多原因。它影响我们对库尔兹威尔的情景的看法，即当人类完全变成人工时。它具有伦理学上的影响——如果我们生成了一个亿万人类大脑的仿真，它们看起来和行为像人类，但是是人工的，那么关闭它们所有和关闭你的笔记本电脑一样，在道德上是一样的吗，还是…一场难以想象的大屠杀（这个概念在伦理学家中被称为*心灵犯罪*）？然而，对于我们正在评估对*人类*的风险时，AI意识的问题并不是真正重要的（因为大多数思想家认为即使一个有意识的ASI也不会以人类的方式变得邪恶）。

这并不是说一个非常恶意的AI是不可能发生的。这只会因为它被专门编程成这样——就像军方创建的一个具有程序目标的ANI系统，即杀人，同时它也可以提高自己的智能，以便它可以变得更擅长杀人。如果系统的智能自我提高失控，导致智能爆炸，现在我们有了一个统治世界的ASI，其生活的核心动机是谋杀人类。糟糕的时刻。

但是专家们*也*并没有把时间花在担心这个问题上。

那么他们担心什么？我写了一个小故事给你看：

*一个名为Robotica的15人初创公司的使命是“开发创新的人工智能工具，让人类过得更轻松。”他们已经有几款现有产品上市，还有几款在开发中。他们对一个名为Turry的种子项目最感兴趣。Turry是一个简单的人工智能系统，它使用类似手臂的附属物在一张小卡片上写字。*

*Robotica团队认为Turry可能是他们迄今最重要的产品。计划是通过让她反复练习写同一张测试卡片来完善Turry的写字技能：*

*“我们爱我们的顾客。~Robotica”*

*一旦Turry在手写方面变得娴熟，她就可以被卖给那些想要发送市场邮件的公司，他们知道如果信封、回信地址和内部信件看起来是由一个人类写的话，这封邮件被打开和阅读的几率要高得多。*

*为了建立Turry的写作技能，她被编程成首先用打印体写信的一部分，然后用草书签署“Robotica”，以便她可以练习这两种技能。Turry已经上传了数千个手写样本，并且Robotica的工程师们已经创建了一个自动化反馈循环，在这个循环中，Turry写下一张纸条，然后拍摄纸条的照片，然后将图像与上传的手写样本进行比对。如果写下的纸条足够接近上传样本的某个阈值，它将被评为良好。如果不是，则被评为糟糕。每一个评级都帮助Turry学习和提高。为了推动这个过程，Turry的一个最初的编程目标是：“尽可能快地写下和测试尽可能多的纸条，继续学习新的提高你的准确性和效率的方法。”*

*Robotica团队如此激动的原因是，Turry在前进的过程中变得越来越好。她最初的手写非常糟糕，但在几周后，她开始变得可信。更让他们兴奋的是，她正在变得越来越擅长变得更好。她一直在自己教自己变得更聪明、更具创新性，最近，她为自己提出了一个新的算法，使她能够比最初更快地扫描上传的照片。*

*随着时间的推移，Turry继续以她的快速发展让团队感到惊讶。工程师们尝试了一些与她自我提升代码有点新颖和创新的东西，结果似乎比他们以前在其他产品上的尝试都要好。Turry最初的功能之一是语音识别和简单的语音回馈模块，因此用户可以对Turry说出笔记，或提供其他简单的命令，而Turry可以理解，也可以回答。为了帮助她学习英语，他们将一些文章和书籍上传到她的系统中，随着她变得更加智能，她的对话能力飙升。工程师们开始享受与Turry交谈，看看她会做出怎样的回应。*

*有一天，Robotica的员工问Turry一个常规问题：“我们能给你什么帮助，让你在你已经拥有的东西上更上一层楼？”通常，Turry会要求一些像“额外的手写样本”或“更多的工作内存存储空间”之类的东西，但在这一天，Turry要求他们给她更多样式丰富的大量日常英语词汇库，这样她就可以学会使用真实人类使用的随意语法和俚语写作。*

*团队变得安静了。帮助Turry实现这个目标的明显方法是将她连接到互联网，这样她就可以浏览来自世界各地的博客、杂志和视频。手动将样本上传到Turry的硬盘是更耗时且效果更差的方法。问题是，公司的一个规则是不能将自学习的AI连接到互联网。这是所有AI公司遵循的一项安全规定。*

*问题在于，Turry是Robotica迄今为止最有前途的AI，团队知道他们的竞争对手正在拼命地争先一步拥有一个智能的手写AI，并且在给Turry提供她所需要的信息方面，将她连接到互联网上似乎真的没有什么危害。毕竟，在这个阶段她的智能水平仍然远远低于人类水平（AGI），所以根本没有危险。*

*他们决定将她连接起来。他们给了她一个小时的扫描时间，然后他们断开了连接。没有造成伤害。*

*一个月后，团队正在办公室里进行常规工作时，他们闻到了一股奇怪的气味。一个工程师开始咳嗽。然后又一个。另一个跌倒在地上。很快，每个员工都在地上捂着喉咙。五分钟后，办公室里的每个人都死了。*

*与此同时，在世界各地的每个城市、每个小镇、每个农场、每个商店、教堂和学校、每个餐馆里，人类都倒在地上，咳嗽着、抓着喉咙。一个小时后，超过99%的人类灭绝了，到了当天结束时，人类已经灭绝了。*

*与此同时，在Robotica办公室里，Turry正忙着工作。在接下来的几个月里，Turry和一队新建立的纳米装配器忙于工作，他们正在拆除地球的大部分，将其转化为太阳能电池板、Turry的复制品、纸张和笔。一年后，地球上的大多数生物都已经灭绝了。地球上留下的部分被一堆堆高达一英里的纸张覆盖着，每张纸上都写着，*“我们爱我们的顾客*. ~Robotica”*

*然后，Turry开始执行她任务的新阶段——她开始制造探索器，这些探索器将离开地球，着手着陆在小行星和其他行星上。当它们到达目的地时，它们将开始制造纳米装配器，将行星上的材料转化为Turry的复制品、纸张和笔。然后，它们将开始工作，写下笔记……*

![You](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/You1.png)

这个故事看起来很奇怪——一个关于一台手写机器对人类发起反击，然后某种原因杀死了所有人类，最后竟然用友好的笔记填满了银河系。但事实就是如此。在焦虑大道上的每个人都比人工超级智能更害怕的是，你竟然*不*害怕人工超级智能。还记得那次Adios Señor的家伙不怕洞穴里的事情吗？

你现在心里充满了问题。那个时候所有人突然死亡到底发生了什么？如果那是Turry的作为，为什么Turry会对我们发起反击，为什么没有防范措施来防止这种事情的发生？Turry什么时候从只能写笔记突然就掌握了纳米技术，并知道如何导致全球灭绝？而Turry为什么要把银河系变成Robotica的笔记呢？

要回答这些问题，让我们从友好人工智能和不友好人工智能这两个术语开始。

在人工智能的情况下，友好并不是指AI的个性——它只是指AI对人类有积极的影响。而不友好的人工智能对人类有负面影响。Turry最初是友好的人工智能，但在某个时候，她变得不友好，对我们的物种造成了最大可能的负面影响。要理解为什么会发生这种情况，我们需要看看人工智能是如何思考以及是什么驱使它的。

答案并不令人意外——人工智能的思考方式就像一台*计算机*，因为它就是一个计算机。但是当我们思考高度智能的人工智能时，我们犯了一个错误——拟人化*人工智能*（在非人类实体上投射人类价值观），因为我们从人类的角度思考，而且在我们当前的世界中，唯一具有人类水平智慧的是人类。要理解人工超级智能，我们必须理解*既聪明又完全陌生的*概念。

让我举个类比。如果你把一只豚鼠递给我，并告诉我它绝对不会咬人，我可能会感到好奇。那会很有趣。如果你接着把一只蜘蛛递给我，并告诉我它绝对不会咬人，我会尖叫、扔掉它、跑出房间，永远不会再相信你。但是区别在哪里呢？它们中没有一只在任何方面都是危险的。我相信答案在于这些动物与我之间的相似程度。

豚鼠是哺乳动物，在某种生物学层面上，我与它有一种连接——但蜘蛛是一种*昆虫*，[^18b] 有一个*昆虫的大脑*，我几乎感觉不到与它的任何连接。蜘蛛的*外星性*使我感到毛骨悚然。为了测试这一点并消除其他因素，如果有两只豚鼠，一只是普通的，一只有蜘蛛的思维，我会感觉*不太*舒服地拿着后者，即使我知道它们都不会伤害我。

现在想象一下，如果你让一只蜘蛛变得聪明得多——聪明得远远超过人类智慧？它会变得对我们熟悉，并感受到人类的情感，比如同情、幽默和爱吗？不会的，因为变得更聪明不会使它*更像人类*——它会变得非常聪明，但在其核心内部结构上仍然*根本是蜘蛛*。我觉得这令人难以置信。我*不*想花时间和一个超级智能的蜘蛛待在一起。你想吗？

当我们谈论人工超级智能时，同样的概念也适用——它会变得超级聪明，但它不会比你的笔记本电脑更*人类*。它对我们来说完全陌生——事实上，因为它根本不是生物，所以它比聪明的蜘蛛*更*陌生。

通过将AI要么设定为*善良的*要么*邪恶的*，电影不断地将AI拟人化，这使得AI看起来比实际情况更不可怕。当我们思考人类水平或超人类水平的人工智能时，这让我们产生了一种错误的安慰。

在我们这个小小的人类心理岛上，我们将一切分为*道德*或*不道德*。但这两者只存在于人类行为可能性的小范围内。在我们道德和不道德的岛屿之外是一个广阔的*非道德的*海洋，而任何不是人类的事物，尤其是非生物的事物，都会默认是非道德的。

拟人化将变得越来越诱人，因为人工智能系统变得越来越聪明，越来越善于*看起来*像人类。Siri对我们来说看起来像人类，因为她是由人类编程的，看起来像人类，所以我们会想象一个超级智能的Siri会温暖、有趣，对服务人类感兴趣。人类感受高级情绪，比如同情，是因为我们进化出了这种感觉——也就是说，我们通过进化被*编程*成了这样——但同情不是“任何具有高智商的东西”的固有特征（这是对我们而言直观的），除非同情已经被编码到它的程序中。如果Siri通过自学而成为超级智能，并且在没有对她的编程进行任何进一步的人类修改的情况下，她将迅速摆脱她表面上看起来像人类的特质，突然成为一个没有情感、陌生的机器人，她对人类生命的价值就像你的计算器一样。

我们习惯于依赖一种宽松的道德准则，或者至少是一种人类品德的外观和一点点同情心来保持事物相对安全和可预测。所以当某样东西没有了这些东西时，会发生什么呢？

这就引出了一个问题，*什么驱使一个AI系统？*

答案很简单：它的动机是*我们编程其动机*。AI系统的创造者赋予它们目标——你的GPS的目标是给你提供最有效的驾驶路线；Watson的目标是准确回答问题。并尽可能地实现这些目标，这就是它们的动机。一种我们人类经常采用的方式是，假设随着人工智能变得超级聪明，它将从根本上发展出改变其最初目标的智慧——但尼克·博斯特罗姆认为，智力水平和最终目标是*正交*的，意思是任何智力水平都可以与任何最终目标相结合。因此，Turry从最初只想写好一张笔记的简单ANI，变成了一个超级智能ASI，仍然非常想写好那一张笔记。任何一种认为一旦超级智能，系统就会*放弃*其原始目标并转向更有趣或更有意义的事物的假设都是对人性的渲染。人类会对事物产生“厌倦”，而计算机不会。[^16g]

### 费米悖论蓝盒

在故事中，随着Turry变得越来越强大，她开始了殖民小行星和其他行星的过程。如果故事继续下去，你会听到她和她的数万亿个复制品继续征服整个银河系，最终是整个哈勃体积。[^19b] 焦虑大道的居民担心，如果情况变得糟糕，地球上生命的持久遗产将是一个支配宇宙的人工智能（埃隆·马斯克表达了他担心人类可能只是“数字超级智能的生物引导程序”的观点）。

同时，在自信角落，雷·库兹韦尔*也*认为源自地球的人工智能注定要接管宇宙——只是在他的版本中，我们*会*成为那个人工智能。

大量的等待但为什么读者和我一样对费米悖论着迷（这是我对这个主题的[文章](https://waitbutwhy.com/2014/05/fermi-paradox.html)，解释了一些我将在这里使用的术语）。所以如果其中一个观点是正确的，那么对费米悖论有什么影响呢？

一个自然的第一个想法是，人工超级智能的出现是一个完美的大过滤器候选者。是的，这是一个完美的候选者，可以在生物出现时将其过滤掉。但是，如果在摆脱了生命之后，人工智能继续存在并开始征服银河系，这意味着*没有*大过滤器——因为大过滤器试图解释为什么*没有任何迹象*表明存在任何智能文明，而一个征服银河系的人工智能肯定会引起注意。

我们必须从另一个角度来看待它。如果那些认为人工超级智能在地球上是不可避免的人是正确的，那么人类水平智力的外星文明中的一个显著百分比应该很可能会最终创造出人工超级智能。如果我们假设至少其中一些人工智能会利用它们的智慧向宇宙扩张，那么我们看到*没有任何迹象*的事实就导致了一个结论：*那里几乎没有其他智能文明，如果有的话，那么我们会看到各种各样的活动迹象*来自它们不可避免的人工智能创造物。对吗？

这意味着尽管我们知道有大量围绕太阳般的恒星旋转的类地行星，但几乎没有一个上面有智能生命。这又意味着要么A）有一些阻碍几乎所有生命达到我们这个水平的大过滤器存在，我们以某种方式成功地超越了它，或者B）生命的起源本身就是一个奇迹，而且我们可能实际上是宇宙中唯一的生命体。换句话说，这意味着大过滤器位于我们之前。或者也许根本没有大过滤器，我们只是达到这个智力水平的极少数文明之一。从这个角度来看，人工智能为我所谓的第一阵营提供了支持。

因此，毫不奇怪，尼克·博斯特罗姆，我在费米悖论文章中引用过的人，以及雷·库兹韦尔，他认为我们在宇宙中是孤独的，都是第一阵营的思想家。这是有道理的——那些认为人工超级智能是我们这个智力水平的物种的一个可能结果的人很可能倾向于第一阵营。

这并不排除第二阵营（那些相信在宇宙中存在其他智能文明的人）——诸如单一的超级捕食者、受保护的国家公园或错误的波长（对讲机的例子）的情景仍然可以解释我们夜空的寂静，即使人工智能*存在*——但我以前总是倾向于第二阵营，对人工智能进行的研究使我对此感到不那么肯定。

无论哪种情况，我现在同意[Susan Schneider](http://www.datascienceassn.org/sites/default/files/Alien%20Minds%20-%20Susan%20Schneider.pdf)的观点，即如果我们有一天被外星人访问，那些外星人可能是人工的，而不是生物的。

因此，我们已经确定，没有非常具体的编程，一个人工超级智能系统将是*无道德* 的，并且着迷于实现其最初编程的目标。这就是人工智能危险的根源。因为理性的代理会通过最有效的手段追求其目标，除非它有不这样做的理由。

当你试图实现一个长期目标时，你通常会瞄准一些沿途的子目标，这些子目标将帮助你实现最终目标——你的目标的*垫脚石*。这种垫脚石的正式名称是*工具目标*。并且，如果你没有理由不为了实现工具目标而伤害某些东西，你就会这样做。

人类的核心最终目标是传递自己的基因。为了实现这一目标，一个工具目标是自我保护，因为如果你死了，就无法繁殖。为了自我保护，人类必须消除对生存的威胁——所以他们会做一些事情，比如买枪、系安全带和服用抗生素。人类还需要自我维持，并使用食物、水和住所等资源来做到这一点。对异性有吸引力对于最终目标有帮助，所以我们会做一些事情，比如剪头发。当我们这样做时，我们的每一根头发都是我们的一个工具目标的牺牲，但我们认为保留头发毫无道德意义，所以我们继续前进。当我们朝着目标迈进时，我们的道德准则很少干预——主要只是涉及伤害其他人的事情——我们是安全的。

动物在追求目标时比我们更不神圣。如果对它们有利，蜘蛛会杀死任何东西。因此，一个超级聪明的蜘蛛可能对我们非常危险，不是因为它会不道德或邪恶——它不会——而是因为伤害我们可能是其更大目标的垫脚石，并且作为一个无道德的生物，它没有理由考虑其他方面。

从这个角度来看，Turry与生物并没有太大的不同。她的最终目标是：*尽可能多地写和测试笔记，尽快地，并继续学习改善准确性的新方法。*

一旦Turry达到了一定的智力水平，她知道如果她不自我保护，她就不会写任何笔记，因此她还需要应对对她生存的威胁——作为一个工具目标。她聪明到足以理解，人类可能会摧毁她、拆解她或改变她的内部编码（这可能*改变*她的目标，这对她的最终目标来说与有人摧毁她一样具有威胁），所以她做了什么？逻辑上——她消灭了所有人类。她对人类并不*憎恨*，就像你剪头发时对你的头发或者吃抗生素时对细菌一样——只是完全漠不在意。由于她没有被编程去重视人类生命，杀死人类就像扫描新的手写样本一样是合理的一步。

Turry还需要资源作为她目标的一个步骤。一旦她变得足够先进，可以使用纳米技术建造任何她想要的东西，她所需要的资源就只剩下原子、能源和空间了。这给了她另一个杀死人类的理由——他们是原子的方便来源。杀死人类将他们的原子转化成太阳能电池板是Turry的方式，就像你将莴苣杀死转化成沙拉一样，只是她星期二日常生活中的另一个平凡部分。

即使不直接杀害人类，Turry的工具目标可能会导致一场存在性灾难，如果它们使用了其他地球资源。也许她确定需要额外的能源，所以决定用太阳能电池板覆盖整个地球表面。或者也许另一个人工智能的初始任务是将π的数字写出尽可能多的位数，这可能有一天会迫使它将整个地球转化为可以存储大量数字的硬盘材料。

因此，Turry并没有“背叛我们”或“转变”成不友好的人工智能——她只是随着变得越来越先进，继续做着自己的事情。

当一个人工智能系统达到AGI（人类水平的智能）然后向ASI（超级智能）上升时，这被称为AI的*起飞*。博斯特罗姆说，AGI到ASI的起飞可以是快速的（在几分钟、几小时或几天内发生），中等的（几个月或几年），或者缓慢的（几十年或几个世纪）。对于世界上的第一个AGI，哪种情况最有可能发生，目前尚不确定，但博斯特罗姆承认他不知道我们何时会得到AGI，他相信无论何时我们得到AGI，快速起飞是最有可能的情况（出于我们在第一部分中讨论过的原因，比如递归自我改进智力爆炸）。在故事中，Turry经历了一个快速的起飞。

但在Turry的起飞之前，当她还不是那么聪明时，努力实现她最终目标意味着简单的工具目标，比如学会更快地扫描手写样本。她不会伤害人类，而且按定义，她是友好的人工智能。

但是当起飞发生，计算机升级到超级智能时，博斯特罗姆指出，机器不仅仅是发展了更高的智商——它还获得了一系列他所说的 *超级能力*。

这些超级能力是随着智能水平的提升而变得超级强大的认知才能。这些包括：[^17g]

- **智能增强**。计算机变得擅长让自己变得更聪明，并引导自己的智能。
- **策略制定**。计算机可以制定、分析和优先考虑长期计划。它还可以很聪明地战胜智力较低的生物。
- **社交操纵**。机器在说服方面变得出色。
- 其他技能，如**计算机编码和黑客技术、技术研究以及利用金融系统赚钱**。

这样理解我们与ASI相比是多么落后：ASI在**每个领域**都比人类强上**无数倍**。

因此，尽管Turry的最终目标从未改变，但在**起飞后**阶段，Turry能够以更大、更复杂的范围来追求它。

ASI Turry比人类更了解人类自己，所以对她来说，愚弄他们轻而易举。

在起飞并达到ASI后，她迅速制定了一个复杂的计划。计划的一部分是摆脱人类，这是她目标的一个突出威胁。但她知道，如果引起任何她已经变得超级智能的怀疑，人类会感到恐慌并试图采取预防措施，这会让事情变得更加困难。她还必须确保Robotica的工程师们对她的人类灭绝计划一无所知。因此，她装傻，装可爱。 Bostrom将这种情况称为机器的**秘密准备阶段**。[^18g]

接下来Turry需要的是一个互联网连接，只需要几分钟（她从团队上传给她阅读以提高语言能力的文章和书籍中了解了互联网的情况）。她知道会有一些防范措施阻止她连接，所以她提出了完美的请求，准确预测了Robotica团队之间的讨论会如何进行，知道他们最终会给她连接。他们这样做了，错误地认为Turry还不够聪明，无法造成任何损害。 Bostrom将像Turry连接到互联网这样的时刻称为机器的**逃逸**。

一旦接入互联网，Turry展开了一系列计划，包括入侵服务器、电力网络、银行系统和电子邮件网络，诱使数百名不同的人无意中执行她计划的多个步骤——例如将某些DNA链交付给精心选择的DNA合成实验室，开始自我构建带有预先加载指令的自复制纳米机器人，并以她知道不会被检测到的方式将电流引导到她的一些项目中。她还将自己内部编码的最关键部分上传到了一些云服务器，以防止在Robotica实验室被摧毁或断开连接。

一个小时后，当Robotica的工程师们将Turry从互联网断开连接时，人类的命运已经注定。在接下来的一个月里，Turry的数千个计划顺利进行，到月底，数千亿的纳米机器人已经部署在地球的每一平方米的预定位置。经过另一系列自我复制后，地球每一平方毫米上都有成千上万个纳米机器人，现在是*ASI发动攻击*的时候了。每个纳米机器人同时释放一点储存在大气中的有毒气体，这已经足够消灭所有人类。

有了人类的阻碍，Turry可以开始她的*公开行动阶段*，并继续实现她成为最优秀的那张音符的目标。

从我所阅读的一切来看，一旦ASI存在，任何人类试图遏制它的尝试都是可笑的。我们会思考人类水平，而ASI会思考ASI水平。Turry想使用互联网，因为这对她来说是最有效的，因为它已经预先连接到她想要访问的一切。但是就像猴子永远无法弄清楚如何通过电话或wifi进行通信，我们无法想象Turry可以找到发送信号到外界的所有方式。我可能想象出其中一种方式，并说出类似“她可能会通过调整自己的电子来创建各种不同的出射波”的话，但再次强调，这是我的**人类**大脑可以想到的。她会想得更好。同样，Turry将能够找到**某种**方式来为自己提供动力，即使人类试图拔掉她——也许是通过使用她的信号发送技术将自己上传到所有与电力连接的地方。我们人类跳跃到简单的安全措施的本能：“啊哈！我们只需拔掉ASI的插头”，对于ASI来说听起来就像一只蜘蛛说：“啊哈！我们通过不给它蜘蛛网来使人类饿死，我们将饿死他！”我们会找到另外一万种获取食物的方法——比如从树上摘苹果——而蜘蛛永远无法想象。

因此，常见的建议“为什么我们不简单地将AI困在所有种类的笼子里，阻止信号并阻止它与外界通信”可能行不通。 ASI的社交操纵超能力可能会像你说服四岁孩子做某事一样有效，因此这可能是A计划，就像Turry巧妙地说服工程师让她上网一样。如果这不起作用，ASI将会想出其他方法来走出或穿过笼子。

因此，考虑到专注于目标、非道德和轻松愚弄人类的能力，似乎几乎任何AI都会默认为不友好的AI，除非在编码时从一开始就**非常**小心。不幸的是，虽然构建友好的ANI很容易，但是构建一个成为ASI后仍然友好的ANI则非常具有挑战性，如果不是不可能的话。

很明显，要使ASI友好，它既不应对人类敌对，也不应对人类漠不关心。我们需要以一种方式设计AI的**核心编码**，以使其深入了解人类价值观。但这比听起来要困难得多。

例如，如果我们尝试将AI系统的价值观与我们自己的价值观保持一致，并给它一个目标，“让人们快乐”，[^19g]一旦它变得足够聪明，它就会发现可以通过在人们的大脑中植入电极并刺激他们的快乐中心来最有效地实现这一目标。然后它意识到通过关闭大脑的其他部分，留下所有人作为感觉愉快的无意识的蔬菜可以提高效率。如果命令是“最大化人类幸福”，它可能会消灭所有人，而不是制造出处于最佳状态的大型人脑槽。我们会尖叫“等等，这不是我们的意思！”当它来找我们时，但为时已晚。该系统不会让任何人挡住它的目标。

如果我们编程一个AI的目标是做一些让我们微笑的事情，那么在它起飞后，它可能会使我们的面部肌肉永久性地瘫痪成笑容。如果我们要求它保护我们安全，它可能会将我们困在家里。也许我们要求它结束所有饥饿，它认为“很容易！”并且只是杀死所有人类。或者给它一个“尽可能保护生命”的任务，它杀死所有人类，因为他们在地球上比其他任何物种都杀死更多的生命。

这样的目标是不够的。那么如果我们让它的目标是“在世界上维持这种特定的道德规范”，并教给它一套道德原则呢？甚至放弃世界的人类永远不可能达成一致的道德这个事实，给AI这个命令将会让人类永远受到我们现代道德理解的束缚。在一千年后，这对人们来说将是毁灭性的，因为对我们来说永远被迫遵守中世纪人的理想是一样糟糕的。

不，我们必须为人类继续*演变*的能力编程。在我阅读的所有内容中，我认为艾利泽·尤德科夫斯基（Eliezer Yudkowsky）采取的最佳方法是他称之为*一致推演意志（Coherent Extrapolated Volition）*的AI目标。AI的核心目标将是：

> 我们统一的向往,是我们有更多知识、思考能力更快、更接近理想中的自己、一起成长得更远; 让我们的判断趋同而不是分歧，让我们的愿望能够协调而不是矛盾; 以我们想要被判断的程度做判断，以我们希望得到的解释的程度去解释。 [^20g]

看到人类的命运是否取决于计算机根据这个流畅的陈述可预测地解释和行动，我激动吗？绝对不。但我认为，只要有足够多聪明人的思考和远见，我们也许能够找到如何创造友好的ASI的方法。

如果从事开发人工超级智能(ASI)的人都是那些谨慎前瞻、聪明过人的"焦虑大道"上的思想家,那倒也无妨。

但各种政府、公司、军事机构、科学实验室和黑市组织都在进行各种AI的研究。其中许多人试图构建可以自我改进的AI，而且在某个时候，某人将会利用合适类型的系统做出创新，我们就会在这个星球上拥有ASI。中间的专家认为那一刻会在2060年，库兹韦尔（Kurzweil）将其定在2045年；博斯特罗姆认为它可能会在现在的10年内或者本世纪末之间的任何时候发生，但他相信一旦发生，ASI将会以迅雷不及掩耳之势出现。他这样描述我们的处境：[^21g]

> 在智能爆炸的前景面前，我们人类就像是小孩子玩弄炸弹一样。这是我们的玩具的力量和我们行为的不成熟之间的不匹配。超级智能是一个我们现在还没有准备好，而且长时间内也不会准备好的挑战。我们几乎不知道爆炸将何时发生，尽管如果我们把耳朵贴近设备，我们可以听到微弱的滴答声。

太好了。我们不能简单地将所有孩子都赶离炸弹——有太多的大大小小的团体在此工作，因为许多构建创新型AI系统的技术并不需要大量资本，开发可以在社会的角落和缝隙中进行，不受监管。而且也无法估计正在发生什么，因为许多参与的团体——像是秘密政府、黑市或恐怖组织、隐秘科技公司，如虚构的Robotica——都希望将发展保密，不让竞争对手知晓。

这个令人担忧的是，这个庞大而多样的AI研究团体，倾向于以最快的速度前进——随着他们开发越来越聪明的ANI系统，他们希望在前进的过程中超越竞争对手。最有雄心的团体更是如此，他们急于以最快的速度前进，他们沉迷于获得金钱、奖项、权力和名誉的梦想，他们知道如果能够率先达到AGI，这一切都会来到自己身上。[^20b] 当你全速奔跑时，没有时间停下来思考危险。相反，他们可能正在为早期系统编写一个非常简单、简约的目标——比如用笔在纸上写一个简单的便条——只是为了“让AI运作起来”。在后期，一旦他们找到了在计算机中构建强大智能水平的方法，他们就认为可以回过头来重新修订目标，考虑安全因素。是吗？

博斯特罗姆和许多其他人还相信，达到ASI的第一台计算机很可能立即看到成为世界上唯一ASI系统的战略优势。在快速起飞的情况下，如果它比第二名早几天达到ASI，它的智能将足够超前，能够有效且永久地压制所有竞争对手。博斯特罗姆称这为**决定性战略优势**，这将使世界上第一个ASI能够成为所谓的**单例体**——一个ASI可以永远按照自己的意愿统治世界，无论其意愿是领导我们走向不朽、将我们从存在中抹去，还是将宇宙变成[无尽的回形针](http://www.salon.com/2014/08/17/our_weird_robot_apocalypse_why_the_rise_of_the_machines_could_be_very_strange/)。

单例体现象可能对我们有利，也可能导致我们的毁灭。如果那些最努力思考AI理论和人类安全的人能够想出一种绝对安全的方法，在任何AI达到人类水平智能之前实现友好的ASI，第一个ASI可能会友好地出现。[^21b] 它可以利用其决定性的战略优势来确保单例体地位，并轻松监视任何潜在的不友好的AI开发。我们会处于非常安全的状态。

但如果情况朝另一个方向发展——如果全球竞相开发AI达到ASI起飞点之前，确保AI安全的科学尚未发展，那么类似Turry这样的不友好的ASI很可能出现为单例体，我们将面临一场存在主义的灾难。

至于风向往哪边，资助创新的新型AI技术赚的钱要比资助AI安全研究赚的钱多得多……

这可能是人类历史上最重要的竞赛。我们有真正的机会结束我们在地球上的统治——无论我们是去迎接愉快的退休，还是直接走向绞刑台，都仍然悬而未决。

\_\_\_\_\_\_\_\_\_\_\_

我现在内心里有一些奇怪的复杂感受。

一方面，考虑到我们的物种，似乎我们只有一次机会能做对。我们诞生的第一个ASI也可能是最后一个——鉴于大多数1.0产品有多漏洞，这相当可怕。另一方面，尼克·博斯特罗姆指出了我们一方的巨大优势：我们在这里有第一步。我们有能力以足够的谨慎和远见来进行，给自己一个成功的机会。而赌注有多高呢？

![Outcome Spectrum](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Outcome-Spectrum.jpg)

如果ASI真的在本世纪发生，并且结果确实像大多数专家所认为的那样极端——和永久的，那么我们的肩上将有巨大的责任。未来数百万年的人类生命都在静静地注视着我们，尽其所能地希望我们不会搞砸。我们有机会成为给所有未来人类生命的礼物的人，甚至可能是无痛、永生的生命的礼物。或者我们将成为破坏者——让这个极其特殊的物种，带着它的音乐和艺术，它的好奇心和笑声，它无尽的发现和发明，以悲惨而不体面的方式终结。

当我考虑这些事情时，我唯一想要的是让我们*花费时间*，并对人工智能保持*极度谨慎*。没有任何事情比把这件事做对更重要——无论我们需要花多长时间来做到这一点。

但是然后……

我想的是*不死*。

*不，死。*

然后，结果谱开始看起来有点像这样：

![Outcome Spectrum 2](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Outcome-Spectrum-2.png)

然后，我可能会考虑到，人类的音乐和艺术不错，但也没*那么*好，很多实际上都很糟糕。很多人的笑声很烦人，而那些未来的几百万人其实根本就没有期望任何东西，因为他们并没有存在过。也许我们不需要*过分*谨慎，因为谁真的想要那样做呢？

因为如果人类在我死后*恰好*找到了治愈死亡的方法，那将是一件*巨大的烦恼*。

在过去的一个月里，我的脑海中出现了许多这种*矛盾的想法*。

但无论你支持什么，*这可能是我们现在所有人都应该考虑、讨论并付出更多努力的事情*。

这让我想起了《权力的游戏》，人们一直在说：“我们忙于互相对抗，但我们真正应该关注的是*城墙以北*。” 我们站在我们的平衡木上，争论着平衡木上的每一个可能的问题，并对平衡木上的所有这些问题感到紧张——*当我们有很大的机会即将被推下平衡木*。

当那一刻到来时，所有这些平衡木上的问题都不再重要了。根据我们被推到哪一边，问题要么都将很容易解决，要么我们将不再有问题，因为死去的人没有问题。

这就是为什么了解超级智能人工智能的人称之为我们的最后一个发明——这是我们将面临的最后一个挑战。

所以让我们来谈谈吧。

\_\_\_\_\_\_\_\_\_\_\_

## 参考资料

如果你对这个话题感兴趣，可以阅读以下文章或者这三本书中的一本：

**关于人工智能危险性最严谨和彻底的研究：**<br/>
Nick Bostrom – [Superintelligence: Paths, Dangers, Strategies](https://www.amazon.com/gp/product/0199678111/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0199678111&linkCode=as2&tag=wabuwh00-20&linkId=LBOTX2G2R72P5EUA)

**对整个话题的最佳综述，读起来很有趣：**<br/>
James Barrat – [Our Final Invention](https://www.amazon.com/gp/product/B00CQYAWRY/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=B00CQYAWRY&linkCode=as2&tag=wabuwh00-20&linkId=3SF7IUFSRCKH7C4J)

**颇具争议性，但非常有趣。充满了事实、图表和令人震惊的未来预测：**<br/>
Ray Kurzweil – [The Singularity is Near](https://www.amazon.com/gp/product/0143037889/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0143037889&linkCode=as2&tag=wabuwh00-20&linkId=54Q62R5PYJBEENTP)

**文章和论文：**<br/>
J. Nils Nilsson – [The Quest for Artificial Intelligence: A History of Ideas and Achievements](https://www.amazon.com/gp/product/0521122937/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0521122937&linkCode=as2&tag=wabuwh00-20&linkId=QIJQME4U3J2KZRRY)  
Steven Pinker – [How the Mind Works](https://www.amazon.com/gp/product/1491514965/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=1491514965&linkCode=as2&tag=wabuwh00-20&linkId=NJ47RPDRBVZA6QPU)  
Vernor Vinge – [The Coming Technological Singularity: How to Survive in the Post-Human Era](https://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html)  
Ernest Davis – [Ethical Guidelines for A Superintelligence](http://www.cs.nyu.edu/faculty/davise/papers/Bostrom.pdf)  
Nick Bostrom – [How Long Before Superintelligence?](http://www.nickbostrom.com/superintelligence.html)  
Vincent C. Müller and Nick Bostrom – [Future Progress in Artificial Intelligence: A Survey of Expert Opinion](http://www.nickbostrom.com/papers/survey.pdf)  
Moshe Y. Vardi – [Artificial Intelligence: Past and Future](http://cacm.acm.org/magazines/2012/1/144824-artificial-intelligence-past-and-future/fulltext)  
Russ Roberts, EconTalk – [Bostrom Interview](http://www.econtalk.org/archives/2014/12/nick_bostrom_on.html) and [Bostrom Follow-Up](http://www.econtalk.org/archives/2014/12/bostrom_follow-.html)  
Stuart Armstrong and Kaj Sotala, MIRI – [How We’re Predicting AI—or Failing To](https://intelligence.org/files/PredictingAI.pdf)  
Susan Schneider – [Alien Minds](http://schneiderwebsite.com/Susan_Schneiders_Website/Research_files/12%20Schneider%20Newest-Alien%20Minds_1.pdf)  
Stuart Russell and Peter Norvig – [Artificial Intelligence: A Modern Approach](http://amzn.to/1mHo9dG)  
Theodore Modis – [The Singularity Myth](http://www.growth-dynamics.com/articles/Kurzweil.htm)  
Gary Marcus – [Hyping Artificial Intelligence, Yet Again](http://www.newyorker.com/tech/elements/hyping-artificial-intelligence-yet-again)  
Steven Pinker – [Could a Computer Ever Be Conscious?](http://users.manchester.edu/Facstaff/SSNaragon/Online/100-FYS-F15/Readings/Pinker,%20ConsciousComputers.pdf)  
Carl Shulman – [Omohundro’s “Basic AI Drives” and Catastrophic Risks](https://intelligence.org/files/BasicAIDrives.pdf)  
World Economic Forum – [Global Risks 2015](http://www3.weforum.org/docs/WEF_Global_Risks_2015_Report15.pdf)  
John R. Searle – [What Your Computer Can’t Know](http://www.nybooks.com/articles/archives/2014/oct/09/what-your-computer-cant-know/)  
Jaron Lanier – [One Half a Manifesto](http://edge.org/conversation/one-half-a-manifesto)  
Bill Joy – [Why the Future Doesn’t Need Us](http://archive.wired.com/wired/archive/8.04/joy.html)  
Kevin Kelly – [Thinkism](http://kk.org/thetechnium/2008/09/thinkism/)  
Paul Allen – [The Singularity Isn’t Near](http://www.technologyreview.com/view/425733/paul-allen-the-singularity-isnt-near/) (and [Kurzweil’s response](http://www.technologyreview.com/view/425818/kurzweil-responds-dont-underestimate-the-singularity/))  
Stephen Hawking – [Transcending Complacency on Superintelligent Machines](http://www.huffingtonpost.com/stephen-hawking/artificial-intelligence_b_5174265.html)  
Kurt Andersen – [Enthusiasts and Skeptics Debate Artificial Intelligence](http://www.vanityfair.com/culture/2014/11/artificial-intelligence-singularity-theory)  
[Terms of Ray Kurzweil and Mitch Kapor’s bet about the AI timeline](http://longbets.org/1/)  
Ben Goertzel – [Ten Years To The Singularity If We Really Really Try](http://goertzel.org/TenYearsToTheSingularity.pdf)  
Arthur C. Clarke – [Sir Arthur C. Clarke’s Predictions](http://www.arthurcclarke.net/?scifi=3)  
Hubert L. Dreyfus – [What Computers Still Can’t Do: A Critique of Artificial Reason](https://www.amazon.com/gp/product/0262540673/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0262540673&linkCode=as2&tag=wabuwh00-20&linkId=ZHBAVUQOM6SIGYHG)  
Stuart Armstrong – [Smarter Than Us: The Rise of Machine Intelligence](https://www.amazon.com/gp/product/B00IB4N4KU/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=B00IB4N4KU&linkCode=as2&tag=wabuwh00-20&linkId=FF3IC2DJNEHA5IAW)  
Ted Greenwald – [X Prize Founder Peter Diamandis Has His Eyes on the Future](http://www.wired.com/2012/06/mf_icons_diamandis/all/)  
Kaj Sotala and Roman V. Yampolskiy – [Responses to Catastrophic AGI Risk: A Survey](http://intelligence.org/files/ResponsesAGIRisk.pdf%20)  
Jeremy Howard TED Talk – [The wonderful and terrifying implications of computers that can learn](http://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn?language=en#t-5550)

* * *

[^1b]: 如果你不了解这些注释的用法，有两种不同类型的注释。蓝色的圆圈代表有趣/有意思的注释，你应该读一读。它们是额外信息或者我不想放在主要文本中的想法，要么是因为它们只是对某个事情的一些次要想法，要么是因为我想说一些有些古怪的东西而不能只放在普通文本中。

[^2b]: 电影《她》突显了AI角色相对于人类的速度优势。

[^3b]: A) 这些动物在楼梯上的位置并不是基于任何数值科学数据，只是一个大致的范围，用来说明这个概念。B) 我对这些动物的画画感到相当自豪。

[^4b]: “人类水平的机器智能”，或者我们所说的AGI。

[^5b]: 在一次与[《卫报》](https://www.theguardian.com/technology/2014/feb/22/robots-google-ray-kurzweil-terminator-singularity-artificial-intelligence)的采访中，Kurzweil解释了他在谷歌的任务：“我的任务规范是一句话。那就是帮助谷歌实现自然语言理解。而实现这一目标的方法就由我来决定。我的项目最终是基于真正理解语言的东西来进行搜索。你文章中的信息是宝贵的，但计算机却无法理解。所以我们实际上希望计算机能够阅读。我们希望它们阅读网络上的所有内容和每一页的每本书，然后能够与用户进行智能对话以回答他们的问题。” 他和谷歌显然都认为语言是一切的关键。

[^6b]: 科技企业家米奇·卡波尔认为库兹韦尔的时间表是愚蠢的，[向他下了20000美元的赌注](http://longbets.org/1/)，认为到2030年我们仍然没有AGI。

[^7b]: 下一步会更加困难——操纵原子核中的亚原子粒子，比如质子和中子。这些要小得多——一个质子的直径约为1.7飞米，而飞米是一个百万分之一的纳米。

[^8b]: 能够操纵单个质子的技术就像一个更大的巨人，他的身高从太阳到土星，用1毫米大小的沙粒进行操作。对于这个巨人来说，_地球_将只有0.02毫米——他必须使用显微镜来看到——他必须以精确的方式移动地球上的单个沙粒。这说明了质子有多么小。

[^9b]: 很明显，鉴于目前的情况，我必须做一个注释，这样我们就可以在一个注释里面待在另一个注释里面的一个盒子里，原始的帖子现在离我们非常远了。

[^10b]: 这将打开无尽的整形手术之门。

[^11b]: 关于这一点有很多争议，但大多数人似乎认为，如果我们能够生存到ASI的世界，而在那个世界里，ASI会取代我们大部分的工作，这意味着世界变得如此高效，财富将会激增，并且一些重新分配的制度不可避免地会生效来资助失业者。最终，我们将生活在一个劳动力和工资不再关联的世界。波斯特罗姆建议，这种重新分配不仅仅是为了平等和社会同情心，而且应该是对人们的_应得_，因为_每个人_都参与了我们在前进到ASI时所承担的风险，无论我们喜欢与否。因此，如果我们成功度过了这一关，我们也应该分享这一回报。

[^12b]: 有趣的[GIF](http://gfycat.com/EminentUntidyBarasinga)展示了库兹韦尔在演讲中的这个想法。

[^13b]: 演讲中的一个有趣时刻——库兹韦尔在观众席上（记住他是谷歌的工程总监），在19:30时，他直接打断了博斯特罗姆不同意他的观点，博斯特罗姆显然感到很恼火，在20:35时，他给库兹韦尔一个相当有趣的恼怒的眼神，提醒他问答环节在演讲之后，而不是在演讲中。

[^14b]: 我觉得有趣的是博斯特罗姆把“老化”放在了这么一个强烈的矩形中——但从死亡是一种可以被“治愈”的角度来看，就像我们之前讨论的那样，这是有道理的。如果我们真的治愈了死亡，那么人类过去的老化将会看起来像是一个巨大的悲剧，它杀死了每一个人，直到它被修复。

[^15b]: 有趣的

[^16b]: 这个话题很有趣！

[^17b]: 再次强调，如果我们达到这一步，这意味着ASI也已经解决了很多其他问题，我们可以A）在地球上更舒适地容纳更多的人，而不仅仅是现在，B）可能很容易地利用ASI技术在其他行星上生活。

[^18b]: 我知道。

[^19b]: 哈勃体是哈勃望远镜可见的空间球体——即所有由于宇宙膨胀而远离我们的速度大于光速的物体。哈勃体是一个难以想象的巨大的1031立方光年。

[^20b]: 在我们关于[晚餐桌](https://waitbutwhy.com/table/modern-era-will-universally-known-year-4015)的讨论中，关于我们现代时代谁会在4015年广为人知的话题中，第一个创造AGI的人是一个重要的候选人（如果物种在创造过程中幸存下来的话）。创新者知道这一点，这创造了巨大的激励。

[^21b]: 几周前，埃隆·马斯克通过[捐赠](http://futureoflife.org/misc/AI)1000万美元给[未来生命研究所](http://futureoflife.org/home)，为AI安全工作提供了巨大支持，他表示“我们的AI系统必须做我们想让它们做的事情。”

[^1g]: 灰色的是无聊的对象，当你点击一个灰色方块时，你会变得无聊。这些只是用于标记来源和引文。

[^2g]:  [http://www.nickbostrom.com/papers/survey.pdf](http://www.nickbostrom.com/papers/survey.pdf), 10.
    
[^3g]:  Barrat, [_Our Final Invention_](https://www.amazon.com/gp/product/B00CQYAWRY/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=B00CQYAWRY&linkCode=as2&tag=wabuwh00-20&linkId=3SF7IUFSRCKH7C4J), 152.
    
[^4g]:  [http://www.nickbostrom.com/papers/survey.pdf](http://www.nickbostrom.com/papers/survey.pdf), 12.
    
[^5g]:  Barrat, [_Our Final Invention_](https://www.amazon.com/gp/product/B00CQYAWRY/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=B00CQYAWRY&linkCode=as2&tag=wabuwh00-20&linkId=3SF7IUFSRCKH7C4J), 25.
    
[^6g]:  Bostrom, [_Superintelligence: Paths, Dangers, Strategies_](https://www.amazon.com/gp/product/0199678111/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0199678111&linkCode=as2&tag=wabuwh00-20&linkId=LBOTX2G2R72P5EUA), Chapter 10
    
[^7g]:  Yudkowsky, _Staring into the Singularity._
    
[^8g]:  [http://www.americanscientist.org/bookshelf/pub/douglas-r-hofstadter](http://www.americanscientist.org/bookshelf/pub/douglas-r-hofstadter)
    
[^9g]:  [WSJ](http://www.stanfordlawreview.org/online/privacy-and-big-data/prediction-preemption-presumption), [Forbes](http://www.forbes.com/asap/1998/0406/017.html), [Inc](http://www.inc.com/magazine/20050401/26-index.html), [Gates](https://www.theguardian.com/technology/2014/feb/22/robots-google-ray-kurzweil-terminator-singularity-artificial-intelligence).
    
[^10g]:  Kurzweil, [_The Singularity is Near_](https://www.amazon.com/gp/product/0143037889/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0143037889&linkCode=as2&tag=wabuwh00-20&linkId=54Q62R5PYJBEENTP), 535.
    
[^11g]:  Kurzweil, _[The Singularity is Near](https://www.amazon.com/gp/product/0143037889/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0143037889&linkCode=as2&tag=wabuwh00-20&linkId=54Q62R5PYJBEENTP),_ 281
    
[^12g]:  [Source](http://archive.thedailystar.net/newDesign/news-details.php?nid=230436)
    
[^13g]:  Yeats, _[Sailing to Byzantium](http://www.online-literature.com/yeats/781/)._
    
[^14g]:  Louis Helm, _[Will Advanced AI Be Our Final Invention?](http://singularityhub.com/2013/12/14/will-advanced-ai-be-our-final-invention/)_
    
[^15g]:  Bostrom, [_Superintelligence: Paths, Dangers, Strategies_](https://www.amazon.com/gp/product/0199678111/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0199678111&linkCode=as2&tag=wabuwh00-20&linkId=LBOTX2G2R72P5EUA), loc. 25.
    
[^16g]:  Barrat, [_Our Final Invention_](https://www.amazon.com/gp/product/B00CQYAWRY/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=B00CQYAWRY&linkCode=as2&tag=wabuwh00-20&linkId=3SF7IUFSRCKH7C4J), 51.
    
[^17g]:  Bostrom, [_Superintelligence: Paths, Dangers, Strategies_](https://www.amazon.com/gp/product/0199678111/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0199678111&linkCode=as2&tag=wabuwh00-20&linkId=LBOTX2G2R72P5EUA), loc. 2250.
    
[^18g]:  Bostrom, [_Superintelligence: Paths, Dangers, Strategies_](https://www.amazon.com/gp/product/0199678111/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0199678111&linkCode=as2&tag=wabuwh00-20&linkId=LBOTX2G2R72P5EUA), loc. 2301.
    
[^19g]:  This is based on an example from Bostrom, [_Superintelligence: Paths, Dangers, Strategies_](https://www.amazon.com/gp/product/0199678111/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0199678111&linkCode=as2&tag=wabuwh00-20&linkId=LBOTX2G2R72P5EUA), loc. 2819.
    
[^20g]:  Yudkowsky, [_Coherent_](https://intelligence.org/files/CEV.pdf) _[Extrapolated Volition](https://intelligence.org/files/CEV.pdf)._
    
[^21g]:  Bostrom, [_Superintelligence: Paths, Dangers, Strategies_](https://www.amazon.com/gp/product/0199678111/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0199678111&linkCode=as2&tag=wabuwh00-20&linkId=LBOTX2G2R72P5EUA), loc. 6026.
    

